#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞ —Ç–µ–∫—Å—Ç–∞
–í–∫–ª—é—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É
–ê–≤—Ç–æ—Ä: AI Assistant
–í–µ—Ä—Å–∏—è: 2.0
"""

import sys
import os
import sqlite3
import hashlib
import numpy as np
import re
from decimal import Decimal
from typing import List, Tuple, Dict, Optional
from collections import Counter

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ Django –ø—Ä–æ–µ–∫—Ç—É
sys.path.append('Folder')

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'app.settings')

import django
django.setup()

from documents.models import Document
from documents import vector
from documents.sim_cos import calculate_originality_large_texts, generate_hashed_shingles, coef_similarity_hashed

class AdvancedPlagiarismDetector:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–∞—Ö"""
    
    def __init__(self):
        self.shingle_sizes = [1, 3, 5]  # –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —à–∏–Ω–≥–ª–æ–≤
        self.similarity_threshold = 0.6  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.originality_threshold = 85.0  # –ü–æ—Ä–æ–≥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.min_text_length = 100  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        # text = re.sub(r'[^\w\s]', '', text)
        
        return text.strip()
    
    def calculate_text_similarity(self, text1: str, text2: str) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        text1_clean = self.preprocess_text(text1)
        text2_clean = self.preprocess_text(text2)
        
        results = {}
        
        # 1. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —à–∏–Ω–≥–ª–∞–º —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
        for shingle_size in self.shingle_sizes:
            shingles1 = generate_hashed_shingles(text1_clean, shingle_size)
            shingles2 = generate_hashed_shingles(text2_clean, shingle_size)
            
            if shingles1 and shingles2:
                similarity = coef_similarity_hashed(shingles1, shingles2)
                results[f'shingle_{shingle_size}'] = float(similarity)
        
        # 2. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å–ª–æ–≤–∞–º
        words1 = set(text1_clean.split())
        words2 = set(text2_clean.split())
        
        if words1 and words2:
            word_intersection = len(words1 & words2)
            word_union = len(words1 | words2)
            results['word_similarity'] = word_intersection / word_union if word_union > 0 else 0
        
        # 3. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        char1 = Counter(text1_clean)
        char2 = Counter(text2_clean)
        
        char_intersection = sum((char1 & char2).values())
        char_union = sum((char1 | char2).values())
        results['char_similarity'] = char_intersection / char_union if char_union > 0 else 0
        
        # 4. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences1 = [s.strip() for s in re.split(r'[.!?]+', text1_clean) if s.strip()]
        sentences2 = [s.strip() for s in re.split(r'[.!?]+', text2_clean) if s.strip()]
        
        if sentences1 and sentences2:
            sentence_similarities = []
            for sent1 in sentences1:
                for sent2 in sentences2:
                    if sent1 and sent2:
                        sent1_words = set(sent1.split())
                        sent2_words = set(sent2.split())
                        if sent1_words and sent2_words:
                            sim = len(sent1_words & sent2_words) / len(sent1_words | sent2_words)
                            sentence_similarities.append(sim)
            
            results['sentence_similarity'] = np.mean(sentence_similarities) if sentence_similarities else 0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ
        weights = {
            'shingle_1': 0.3,
            'shingle_3': 0.4,
            'shingle_5': 0.2,
            'word_similarity': 0.1
        }
        
        weighted_similarity = 0
        total_weight = 0
        
        for method, weight in weights.items():
            if method in results:
                weighted_similarity += results[method] * weight
                total_weight += weight
        
        results['overall_similarity'] = weighted_similarity / total_weight if total_weight > 0 else 0
        
        return results
    
    def detect_plagiarism(self, document_id: int) -> Dict:
        """–í—ã—è–≤–ª—è–µ—Ç –ø–ª–∞–≥–∏–∞—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        try:
            document = Document.objects.get(id=document_id)
            
            result = {
                'document_id': document_id,
                'document_name': document.name,
                'originality': 0.0,
                'similarity': 0.0,
                'similar_documents': [],
                'is_plagiarized': False,
                'plagiarism_risk': 'low',
                'detailed_analysis': {},
                'status': 'success',
                'message': ''
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ TXT —Ñ–∞–π–ª–∞
            if not document.txt_file:
                result['status'] = 'error'
                result['message'] = 'TXT —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω'
                return result
            
            txt_path = f"Folder/media/{document.txt_file}"
            if not os.path.exists(txt_path):
                result['status'] = 'error'
                result['message'] = f'TXT —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {txt_path}'
                return result
            
            # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            with open(txt_path, 'r', encoding='utf-8') as f:
                document_text = f.read()
            
            if len(document_text) < self.min_text_length:
                result['status'] = 'warning'
                result['message'] = f'–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ({len(document_text)} —Å–∏–º–≤–æ–ª–æ–≤)'
                return result
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            similar_docs = self._find_similar_documents(document)
            
            if not similar_docs:
                result['originality'] = 100.0
                result['similarity'] = 0.0
                result['message'] = '–î–æ–∫—É–º–µ–Ω—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω - –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'
                result['plagiarism_risk'] = 'very_low'
            else:
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫–∞–∂–¥—ã–º –ø–æ—Ö–æ–∂–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
                similarities = []
                similar_texts = []
                detailed_similarities = []
                
                for doc, vector_similarity in similar_docs:
                    if doc.txt_file:
                        similar_txt_path = f"Folder/media/{doc.txt_file}"
                        if os.path.exists(similar_txt_path):
                            with open(similar_txt_path, 'r', encoding='utf-8') as f:
                                similar_text = f.read()
                            
                            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–∂–µ—Å—Ç–∏
                            detailed_sim = self.calculate_text_similarity(document_text, similar_text)
                            
                            similarities.append(detailed_sim['overall_similarity'])
                            detailed_similarities.append(detailed_sim)
                            similar_texts.append(similar_text)
                            
                            result['similar_documents'].append({
                                'id': doc.id,
                                'name': doc.name,
                                'vector_similarity': float(vector_similarity),
                                'text_similarity': detailed_sim['overall_similarity'],
                                'detailed_similarity': detailed_sim,
                                'originality': float(doc.result) if doc.result else 0.0
                            })
                
                if similar_texts:
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                    originality = calculate_originality_large_texts(
                        document_text, 
                        similar_texts, 
                        shingle_size=3
                    )
                    
                    result['originality'] = max(0.0, min(100.0, originality))
                    result['similarity'] = 100 - result['originality']
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∏—Å–∫ –ø–ª–∞–≥–∏–∞—Ç–∞
                    max_similarity = max(similarities) if similarities else 0
                    avg_similarity = np.mean(similarities) if similarities else 0
                    
                    if max_similarity > 0.9:
                        result['plagiarism_risk'] = 'very_high'
                    elif max_similarity > 0.8:
                        result['plagiarism_risk'] = 'high'
                    elif max_similarity > 0.7:
                        result['plagiarism_risk'] = 'medium'
                    elif max_similarity > 0.5:
                        result['plagiarism_risk'] = 'low'
                    else:
                        result['plagiarism_risk'] = 'very_low'
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –ø–ª–∞–≥–∏–∞—Ç–æ–º
                    result['is_plagiarized'] = (
                        result['originality'] < self.originality_threshold or 
                        max_similarity > 0.8
                    )
                    
                    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
                    result['detailed_analysis'] = {
                        'max_similarity': float(max_similarity),
                        'avg_similarity': float(avg_similarity),
                        'similar_documents_count': len(similar_docs),
                        'text_length': len(document_text),
                        'analysis_methods': list(detailed_similarities[0].keys()) if detailed_similarities and len(detailed_similarities) > 0 else []
                    }
                    
                    if result['is_plagiarized']:
                        result['message'] = f'‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –î–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–ª–∞–≥–∏–∞—Ç! –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result["originality"]:.2f}%, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.2f}'
                    else:
                        result['message'] = f'‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result["originality"]:.2f}%, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.2f}'
                else:
                    result['originality'] = 100.0
                    result['similarity'] = 0.0
                    result['message'] = '–î–æ–∫—É–º–µ–Ω—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω - –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'
                    result['plagiarism_risk'] = 'very_low'
            
            return result
            
        except Document.DoesNotExist:
            return {
                'document_id': document_id,
                'status': 'error',
                'message': f'–î–æ–∫—É–º–µ–Ω—Ç —Å ID {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω'
            }
        except Exception as e:
            return {
                'document_id': document_id,
                'status': 'error',
                'message': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}'
            }
    
    def _find_similar_documents(self, document: Document) -> List[Tuple[Document, float]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        similar_docs = []
        
        if not document.vector:
            return similar_docs
        
        try:
            current_vector = document.get_vector_array()
            if current_vector is None:
                return similar_docs
            
            all_docs = Document.objects.exclude(id=document.id).exclude(vector__isnull=True)
            
            for doc in all_docs:
                doc_vector = doc.get_vector_array()
                if doc_vector is not None:
                    similarity = self._cosine_similarity(current_vector, doc_vector)
                    if similarity > self.similarity_threshold:
                        similar_docs.append((doc, similarity))
            
            similar_docs.sort(key=lambda x: x[1], reverse=True)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        
        return similar_docs
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = np.dot(vec1, vec2) / (norm1 * norm2)
            return float(similarity)
        except:
            return 0.0
    
    def generate_detailed_report(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø–ª–∞–≥–∏–∞—Ç–µ"""
        results = []
        
        documents = Document.objects.all().order_by('id')
        
        for doc in documents:
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {doc.name} (ID: {doc.id})")
            result = self.detect_plagiarism(doc.id)
            results.append(result)
            print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['message']}")
            print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_docs = len(results)
        success_docs = [r for r in results if r.get('status') == 'success']
        plagiarized_docs = [r for r in success_docs if r.get('is_plagiarized', False)]
        error_docs = [r for r in results if r.get('status') == 'error']
        warning_docs = [r for r in results if r.get('status') == 'warning']
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
        risk_levels = {
            'very_high': len([r for r in success_docs if r.get('plagiarism_risk') == 'very_high']),
            'high': len([r for r in success_docs if r.get('plagiarism_risk') == 'high']),
            'medium': len([r for r in success_docs if r.get('plagiarism_risk') == 'medium']),
            'low': len([r for r in success_docs if r.get('plagiarism_risk') == 'low']),
            'very_low': len([r for r in success_docs if r.get('plagiarism_risk') == 'very_low'])
        }
        
        avg_originality = np.mean([r.get('originality', 0) for r in success_docs])
        
        report = {
            'summary': {
                'total_documents': total_docs,
                'successful_analyses': len(success_docs),
                'plagiarized_documents': len(plagiarized_docs),
                'error_documents': len(error_docs),
                'warning_documents': len(warning_docs),
                'average_originality': float(avg_originality),
                'plagiarism_rate': (len(plagiarized_docs) / len(success_docs) * 100) if success_docs else 0
            },
            'risk_analysis': risk_levels,
            'detailed_results': results
        }
        
        return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞"""
    print("=== –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –°–ò–°–¢–ï–ú–ê –í–´–Ø–í–õ–ï–ù–ò–Ø –ü–õ–ê–ì–ò–ê–¢–ê ===\n")
    
    detector = AdvancedPlagiarismDetector()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç...")
    report = detector.generate_detailed_report()
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    summary = report['summary']
    print(f"\n=== –°–í–û–î–ù–´–ô –û–¢–ß–ï–¢ ===")
    print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {summary['total_documents']}")
    print(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {summary['successful_analyses']}")
    print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–ª–∞–≥–∏–∞—Ç–æ–º: {summary['plagiarized_documents']}")
    print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {summary['error_documents']}")
    print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏: {summary['warning_documents']}")
    print(f"–°—Ä–µ–¥–Ω—è—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {summary['average_originality']:.2f}%")
    print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –ø–ª–∞–≥–∏–∞—Ç–∞: {summary['plagiarism_rate']:.2f}%")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
    print(f"\n=== –ê–ù–ê–õ–ò–ó –†–ò–°–ö–û–í ===")
    risk_analysis = report['risk_analysis']
    for risk_level, count in risk_analysis.items():
        print(f"{risk_level.replace('_', ' ').title()}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n=== –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===")
    for result in report['detailed_results']:
        if result['status'] == 'success':
            print(f"\nüìÑ –î–æ–∫—É–º–µ–Ω—Ç '{result['document_name']}' (ID: {result['document_id']}):")
            print(f"  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result['originality']:.2f}%")
            print(f"  –°—Ö–æ–∂–µ—Å—Ç—å: {result['similarity']:.2f}%")
            print(f"  –†–∏—Å–∫ –ø–ª–∞–≥–∏–∞—Ç–∞: {result['plagiarism_risk'].replace('_', ' ').title()}")
            print(f"  –°—Ç–∞—Ç—É—Å: {'üö® –ü–õ–ê–ì–ò–ê–¢' if result['is_plagiarized'] else '‚úÖ –û–†–ò–ì–ò–ù–ê–õ'}")
            print(f"  –°–æ–æ–±—â–µ–Ω–∏–µ: {result['message']}")
            
            if result['similar_documents']:
                print(f"  üîç –ü–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
                for sim_doc in result['similar_documents']:
                    print(f"    - {sim_doc['name']} (ID: {sim_doc['id']})")
                    print(f"      –í–µ–∫—Ç–æ—Ä–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {sim_doc['vector_similarity']:.3f}")
                    print(f"      –¢–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {sim_doc['text_similarity']:.3f}")
                    print(f"      –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å:")
                    for method, value in sim_doc['detailed_similarity'].items():
                        print(f"        {method}: {value:.3f}")
        elif result['status'] == 'warning':
            print(f"\n‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{result.get('document_name', 'Unknown')}' (ID: {result['document_id']}): {result['message']}")
        else:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{result.get('document_name', 'Unknown')}' (ID: {result['document_id']}): {result['message']}")

if __name__ == "__main__":
    main()
